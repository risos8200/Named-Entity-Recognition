{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yHnbGR5wpCL8"
   },
   "source": [
    "# CSE 291 Assignment 2 BiLSTM CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rs2O4920pCob"
   },
   "source": [
    "## Download Data/Eval Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hmfarI0hpHj6"
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/sighsmile/conlleval/master/conlleval.py\n",
    "!wget https://raw.githubusercontent.com/tberg12/cse291spr21/main/assignment2/train.data.quad\n",
    "!wget https://raw.githubusercontent.com/tberg12/cse291spr21/main/assignment2/dev.data.quad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0CMvXrmwpNCM"
   },
   "outputs": [],
   "source": [
    "import conlleval\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.vocab import vocab\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "torch.manual_seed(291)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VOBmqHytpTGs"
   },
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GKfmSZs8pPBV"
   },
   "outputs": [],
   "source": [
    "TRAIN_DATA = 'train.data.quad'\n",
    "VALID_DATA = 'dev.data.quad'\n",
    "UNK = '<unk>'\n",
    "PAD = '<pad>'\n",
    "START_TAG = \"<start>\"  # you can add this explicitly or use it implicitly in your CRF layer\n",
    "STOP_TAG = \"<stop>\"    # you can add this explicitly or use it implicitly in your CRF layer\n",
    "\n",
    "\n",
    "def read_conll_sentence(path):\n",
    "    \"\"\" Read a CONLL-format sentence into vocab objects\n",
    "    Args:\n",
    "        :param path: path to CONLL-format data file\n",
    "        :param word_vocab: Vocabulary object for source\n",
    "        :param label_vocab: Vocabulary object for target\n",
    "    \"\"\"\n",
    "    sent = [[], []]\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            if line:\n",
    "                # replace numbers with 0000\n",
    "                word = line[0]\n",
    "                word = '0000' if word.isnumeric() else word\n",
    "                sent[0].append(word)\n",
    "                sent[1].append(line[3])\n",
    "            else:\n",
    "                yield sent[0], sent[1]\n",
    "                sent = [[], []]\n",
    "\n",
    "\n",
    "def prepare_dataset(dataset, word_vocab, label_vocab):\n",
    "    dataset = [\n",
    "      [\n",
    "        torch.tensor([word_vocab.get_stoi().get(word,0) for word in sent[0]], dtype=torch.long),\n",
    "        torch.tensor([label_vocab.get_stoi()[label] for label in sent[1]], dtype=torch.long),\n",
    "      ]\n",
    "      for sent in dataset\n",
    "    ]\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# load a list of sentences, where each word in the list is a tuple containing the word and the label\n",
    "train_data = list(read_conll_sentence(TRAIN_DATA))\n",
    "train_word_counter = Counter([word for sent in train_data for word in sent[0]])\n",
    "train_label_counter = Counter([label for sent in train_data for label in sent[1]])\n",
    "word_vocab = vocab(train_word_counter, specials=(UNK, PAD), min_freq=2)\n",
    "# print(word_vocab.get_stoi())\n",
    "# word_vocab.set_default_index(UNK)\n",
    "# label_vocab = vocab(train_label_counter, specials=(), min_freq=1)\n",
    "label_vocab = vocab(train_label_counter, specials=(START_TAG,STOP_TAG), min_freq=1)\n",
    "train_data = prepare_dataset(train_data, word_vocab, label_vocab)\n",
    "print('Train word vocab:', len(word_vocab), 'symbols.')\n",
    "print('Train label vocab:', len(label_vocab), f'symbols: {list(label_vocab.get_stoi().keys())}')\n",
    "valid_data = list(read_conll_sentence(VALID_DATA))\n",
    "valid_data = prepare_dataset(valid_data, word_vocab, label_vocab)\n",
    "print('Train data:', len(train_data), 'sentences.')\n",
    "print('Valid data:', len(valid_data))\n",
    "\n",
    "print(' '.join([word_vocab.get_itos()[i.item()] for i in train_data[0][0]]))\n",
    "print(' '.join([label_vocab.get_itos()[i.item()] for i in train_data[0][1]]))\n",
    "\n",
    "print(' '.join([word_vocab.get_itos()[i.item()] for i in valid_data[1][0]]))\n",
    "print(' '.join([label_vocab.get_itos()[i.item()] for i in valid_data[1][1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xNNmZx_Uqy7q"
   },
   "source": [
    "## BiLSTMTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a5nVIM_Eq1ZU"
   },
   "outputs": [],
   "source": [
    "# Starter code implementing a BiLSTM Tagger\n",
    "# which makes locally normalized, independent\n",
    "# tag classifications at each time step\n",
    "\n",
    "class BiLSTMTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, tag_vocab_size, embedding_dim, hidden_dim, dropout=0.3):\n",
    "        super(BiLSTMTagger, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tagset_size = tag_vocab_size\n",
    "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim).to(device)\n",
    "        self.bilstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
    "                            num_layers=1, bidirectional=True, batch_first=True).to(device)\n",
    "        self.tag_projection_layer = nn.Linear(hidden_dim, self.tagset_size).to(device)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (torch.randn(2, 1, self.hidden_dim // 2).to(device),\n",
    "                torch.randn(2, 1, self.hidden_dim // 2).to(device))\n",
    "\n",
    "    def compute_lstm_emission_features(self, sentence):\n",
    "        hidden = self.init_hidden()\n",
    "        embeds = self.dropout(self.word_embeds(sentence))\n",
    "        bilstm_out, hidden = self.bilstm(embeds, hidden)\n",
    "        bilstm_out = self.dropout(bilstm_out)\n",
    "        bilstm_out = bilstm_out\n",
    "        bilstm_feats = self.tag_projection_layer(bilstm_out)\n",
    "        return bilstm_feats\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        bilstm_feats = self.compute_lstm_emission_features(sentence)\n",
    "        return bilstm_feats.max(-1)[0].sum(), bilstm_feats.argmax(-1)\n",
    "\n",
    "    def loss(self, sentence, tags):\n",
    "        bilstm_feats = self.compute_lstm_emission_features(sentence)\n",
    "        # transform predictions to (n_examples, n_classes) and ground truth to (n_examples)\n",
    "        return torch.nn.functional.cross_entropy(\n",
    "              bilstm_feats.view(-1, self.tagset_size), \n",
    "              tags.view(-1), \n",
    "              reduction='sum'\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DH7JGSDAruUg"
   },
   "source": [
    "## Train / Eval loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gw2He2cgrrF1"
   },
   "outputs": [],
   "source": [
    "def train(model, train_data, valid_data, word_vocab, label_vocab, epochs, log_interval=25):\n",
    "    losses_per_epoch = []\n",
    "    for epoch in range(epochs):\n",
    "        print(f'--- EPOCH {epoch} ---')\n",
    "        model.train()\n",
    "        losses_per_epoch.append([])\n",
    "        for i, (sent, tags) in enumerate(train_data):\n",
    "            model.zero_grad()\n",
    "            sent, tags = sent.to(device), tags.to(device)\n",
    "            sent = sent.unsqueeze(0)\n",
    "            tags = tags.unsqueeze(0)\n",
    "            loss = model.loss(sent, tags)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            losses_per_epoch[-1].append(loss.detach().cpu().item())\n",
    "            if i > 0 and i % log_interval == 0:\n",
    "                print(f'Avg loss over last {log_interval} updates: {np.mean(losses_per_epoch[-1][-log_interval:])}')\n",
    "\n",
    "        evaluate(model, valid_data, word_vocab, label_vocab,epoch)\n",
    "loss_list=[]\n",
    "best_loss=float('inf')\n",
    "def evaluate(model, dataset, word_vocab, label_vocab,epoch):\n",
    "    model.eval()\n",
    "    global best_loss \n",
    "    losses = []\n",
    "    scores = []\n",
    "    true_tags = []\n",
    "    pred_tags = []\n",
    "    sents = []\n",
    "    for i, (sent, tags) in enumerate(dataset):\n",
    "        with torch.no_grad():\n",
    "            sent, tags = sent.to(device), tags.to(device)\n",
    "            sent = sent.unsqueeze(0)\n",
    "            tags = tags.unsqueeze(0)\n",
    "            losses.append(model.loss(sent, tags).cpu().detach().item())\n",
    "            score, pred_tag_seq = model(sent)\n",
    "            scores.append(score.cpu().detach().numpy())\n",
    "            true_tags.append([label_vocab.get_itos()[i] for i in tags.tolist()[0]])\n",
    "            pred_tags.append([label_vocab.get_itos()[i] for i in pred_tag_seq[0]])\n",
    "            sents.append([word_vocab.get_itos()[i] for i in sent[0]])\n",
    "\n",
    "    print('Avg evaluation loss:', np.mean(losses))\n",
    "    loss_list.append(np.mean(losses))\n",
    "    if np.mean(losses)<best_loss: \n",
    "        best_loss=np.mean(losses)\n",
    "        print('best_loss',best_loss)\n",
    "        torch.save(model, 'baseline/best-model{}.pt'.format(epoch))\n",
    "        torch.save(model.state_dict(), 'baselineparam/best-model-param{}.pt'.format(epoch))\n",
    "    print(conlleval.evaluate([tag for tags in true_tags for tag in tags], [tag for tags in pred_tags for tag in tags], verbose=True))\n",
    "    print('\\n5 random evaluation samples:')\n",
    "    for i in np.random.randint(0, len(sents), size=5):\n",
    "        print('SENT:', ' '.join(sents[i]))\n",
    "        print('TRUE:', ' '.join(true_tags[i]))\n",
    "        print('PRED:', ' '.join(pred_tags[i]))\n",
    "    return sents, true_tags, pred_tags\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TdJsc_y6rxdC"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nVyfoJfZry4-"
   },
   "outputs": [],
   "source": [
    "# Train BiLSTM Tagger Baseline\n",
    "model = BiLSTMTagger(len(word_vocab), len(label_vocab), 128, 256).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "train(model, train_data, valid_data, word_vocab, label_vocab, epochs=12, log_interval=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TdJsc_y6rxdC"
   },
   "source": [
    "## CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(vec):\n",
    "    _, idx = torch.max(vec, 1)\n",
    "    return idx.item()\n",
    "\n",
    "def log_sum_exp(vec):\n",
    "    max_score = vec[0, argmax(vec)]\n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "    return max_score + torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
    "\n",
    "\n",
    "class BiLSTMCRFTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, tag_vocab, embedding_dim, hidden_dim, dropout=0.3):\n",
    "        super(BiLSTMCRFTagger, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tag_vocab = tag_vocab\n",
    "        self.tagset_size = len(tag_vocab)\n",
    "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim).to(device)\n",
    "        self.bilstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=1, bidirectional=True).to(device)\n",
    "        self.tag_projection_layer = nn.Linear(hidden_dim, self.tagset_size).to(device)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.transitions = nn.Parameter(torch.randn(self.tagset_size, self.tagset_size))\n",
    "        self.transitions.data[tag_vocab[START_TAG], :] = -10000\n",
    "        self.transitions.data[:, tag_vocab[STOP_TAG]] = -10000\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
    "\n",
    "\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (torch.randn(2, 1, self.hidden_dim // 2).to(device),\n",
    "                torch.randn(2, 1, self.hidden_dim // 2).to(device))\n",
    "\n",
    "    def compute_lstm_emission_features(self, sentence):\n",
    "        self.hidden = self.init_hidden()\n",
    "        embeds = self.dropout(self.word_embeds(sentence).view(len(sentence), 1, -1))\n",
    "        lstm_out, self.hidden = self.bilstm(embeds, self.hidden)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
    "        lstm_feats = self.hidden2tag(lstm_out)\n",
    "        return lstm_feats\n",
    "    \n",
    "    def score_sent(self, feats, tags):\n",
    "        score = torch.zeros(1)\n",
    "        tags = torch.cat([torch.tensor([self.tag_vocab[START_TAG]], dtype=torch.long), tags])\n",
    "        for i, feat in enumerate(feats):\n",
    "            score = score + \\\n",
    "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
    "        score = score + self.transitions[self.tag_vocab[STOP_TAG], tags[-1]]\n",
    "        return score\n",
    "    \n",
    "    def forward_p_algo(self, feats):\n",
    "        alphas = torch.full((1, self.tagset_size), -10000.)\n",
    "        alphas[0][self.tag_vocab[START_TAG]] = 0.\n",
    "        forward_var = alphas\n",
    "        for feat in feats:\n",
    "            timestep_alphas = [] \n",
    "            for t_next in range(self.tagset_size):\n",
    "                score_e = feat[t_next].view(1, -1).expand(1, self.tagset_size)\n",
    "                score_t = self.transitions[t_next].view(1, -1)\n",
    "                next_tag = forward_var + score_t + score_e\n",
    "                timestep_alphas.append(log_sum_exp(next_tag).view(1))\n",
    "            forward_var = torch.cat(timestep_alphas).view(1, -1)\n",
    "        var_terminal = forward_var + self.transitions[self.tag_vocab[STOP_TAG]]\n",
    "        alpha = log_sum_exp(var_terminal)\n",
    "        return alpha\n",
    "    \n",
    "    def decode(self, feats):\n",
    "        bptr = []\n",
    "        ver_vars = torch.full((1, self.tagset_size), -10000.)\n",
    "        ver_vars[0][self.tag_vocab[START_TAG]] = 0\n",
    "        forward_var = ver_vars\n",
    "        for feat in feats:\n",
    "            bptr_t = [] \n",
    "            ver_vars_t = [] \n",
    "            for t_next in range(self.tagset_size):\n",
    "                next_tag = forward_var + self.transitions[t_next]\n",
    "                best_tag= argmax(next_tag)\n",
    "                bptr_t.append(best_tag)\n",
    "                ver_vars_t.append(next_tag[0][best_tag].view(1))\n",
    "            forward_var = (torch.cat(ver_vars_t) + feat).view(1, -1)\n",
    "            bptr.append(bptr_t)\n",
    "\n",
    "        var_terminal = forward_var + self.transitions[self.tag_vocab[STOP_TAG]]\n",
    "        best_tag = argmax(var_terminal)\n",
    "        path_score = var_terminal[0][best_tag]\n",
    "        best_path = [best_tag]\n",
    "        for bptr_t in reversed(bptr):\n",
    "            best_tag = bptr_t[best_tag]\n",
    "            best_path.append(best_tag)\n",
    "        start = best_path.pop()\n",
    "        assert start == self.tag_vocab[START_TAG] \n",
    "        best_path.reverse()\n",
    "        return path_score, best_path\n",
    "    \n",
    "    def forward(self, sentence): \n",
    "        lstm_feats = self.compute_lstm_emission_features(sentence)\n",
    "        score, tag_seq = self.decode(lstm_feats)\n",
    "        return score, tag_seq\n",
    "        \n",
    "    def loss(self, sentence, tags):\n",
    "        feats = self.compute_lstm_emission_features(sentence)\n",
    "        num_score = self.score_sent(feats, tags)\n",
    "        partition_score = self.forward_p_algo(feats)\n",
    "        return partition_score - num_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_crf(model, train_data, valid_data, word_vocab, label_vocab, epochs, log_interval=25):\n",
    "    losses_per_epoch = []\n",
    "    for epoch in range(epochs):\n",
    "        print(f'--- EPOCH {epoch} ---')\n",
    "        model.train()\n",
    "        losses_per_epoch.append([])\n",
    "        for i, (sent, tags) in enumerate(train_data):\n",
    "            model.zero_grad()\n",
    "            sent, tags = sent.to(device), tags.to(device)\n",
    "            # sent = sent.unsqueeze(0)\n",
    "            # tags = tags.unsqueeze(0)\n",
    "            loss = model.loss(sent, tags)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            losses_per_epoch[-1].append(loss.detach().cpu().item())\n",
    "            if i > 0 and i % log_interval == 0:\n",
    "                print(f'Avg loss over last {log_interval} updates: {np.mean(losses_per_epoch[-1][-log_interval:])}')\n",
    "\n",
    "        evaluate_crf(model, valid_data, word_vocab, label_vocab,epoch)\n",
    "\n",
    "loss_list_crf=[]    \n",
    "best_loss_crf=float('inf')\n",
    "def evaluate_crf(model, dataset, word_vocab, label_vocab,epoch):\n",
    "    model.eval()\n",
    "    global best_loss_crf \n",
    "    losses = []\n",
    "    scores = []\n",
    "    true_tags = []\n",
    "    pred_tags = []\n",
    "    sents = []\n",
    "    for i, (sent, tags) in enumerate(dataset):\n",
    "        with torch.no_grad():\n",
    "            sent, tags = sent.to(device), tags.to(device)\n",
    "            # sent = sent.unsqueeze(0)\n",
    "            # tags = tags.unsqueeze(0)\n",
    "            losses.append(model.loss(sent, tags).cpu().detach().item())\n",
    "            score, pred_tag_seq = model(sent)\n",
    "            scores.append(score.cpu().detach().numpy())\n",
    "            true_tags.append([label_vocab.get_itos()[i] for i in tags.tolist()])\n",
    "            pred_tags.append([label_vocab.get_itos()[i] for i in pred_tag_seq])\n",
    "            sents.append([word_vocab.get_itos()[i] for i in sent])\n",
    "\n",
    "    print('Avg evaluation loss:', np.mean(losses))\n",
    "    loss_list_crf.append(np.mean(losses))\n",
    "    if np.mean(losses)<best_loss_crf: \n",
    "        best_loss_crf=np.mean(losses)\n",
    "        print(best_loss_crf)\n",
    "        torch.save(model, 'baselinecrf/best-model{}.pt'.format(epoch))\n",
    "        torch.save(model.state_dict(), 'baselineparamcrf/best-model-param{}.pt'.format(epoch))\n",
    "    print(conlleval.evaluate([tag for tags in true_tags for tag in tags], [tag for tags in pred_tags for tag in tags], verbose=True))\n",
    "    print('\\n5 random evaluation samples:')\n",
    "    for i in np.random.randint(0, len(sents), size=5):\n",
    "        print('SENT:', ' '.join(sents[i]))\n",
    "        print('TRUE:', ' '.join(true_tags[i]))\n",
    "        print('PRED:', ' '.join(pred_tags[i]))\n",
    "    return sents, true_tags, pred_tags\n",
    "\n",
    "model = BiLSTMCRFTagger(len(word_vocab), label_vocab, 128, 256).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "train_crf(model, train_data, valid_data, word_vocab, label_vocab, epochs=12, log_interval=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list_crf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochlis=[i for i in range(1,len(loss_list_crf)+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.scatter(epochlis,loss_list)\n",
    "plt.scatter(epochlis,loss_list_crf)\n",
    "plt.plot(epochlis,loss_list, label='Baseline BiLSTm')\n",
    "plt.plot(epochlis,loss_list_crf, label='CRF BiLSTM')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('baselineparam/best-model-param4.pt'))\n",
    "evaluate(model, valid_data, word_vocab, label_vocab,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('baselineparamcrf/best-model-param5.pt'))\n",
    "evaluate_crf(model, valid_data, word_vocab, label_vocab,6)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "cse291_assignment2_starter_code.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
